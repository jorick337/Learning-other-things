# ССЫЛКА НА ИСТОЧНИК: https://qudata.com/ml/ru/NN_Base_Torch_NN_Ref.html

#region ИНТЕРФЕЙСЫ ДЛЯ СЛОЕВ(ГОТОВЫЕ)

import torch
# Слои могут быть функциями или классами

# Модуль который содержит функции для слоев(много)
import  torch.nn.functional as F 
# Слой как функция:
N, nX, nY = 1, 2, 3                               # число примеров, входов, выходов
X = torch.ones(N,  nX)                            # [ [1 1] ] - примеры
# Пояснение для весов:
# nY - размерность выходного тензора
# nX - размерность входного тензора
W = torch.ones(nY, nX)                            # [ [1 1] [1 1] [1 1] ] - весы
B = torch.ones(nY)                                # [ 1 1 1 ] - смещение

# linear - преобразует тензор X с весами W и смещением B по формуле:
# Y = X*W.t() + B
# W.t() = [ [1 1 1] [1 1 1] ] - транспонированная версия W(нужна для умножения матриц)
# X*W.t() = (1,2)*(2,3) = (1,3) = [ [1 1] ] * [ [1 1 1] [1 1 1] ] = 
# [ 1*1+1*1 1*1+1*1 1*1+1*1] = [2 2 2]
# X*W.t() + B = [ [2 2 2] ] + [ 1 1 1 ] = [ [3 3 3] ]
Y = F.linear(X, W, B); #print(Y)                 # [ [3. 3. 3.] ]
# Аналоги этой функции:
Y = X.mm(W.t())+B; #print(Y)                    # [ [3. 3. 3.] ]
Y = torch.addmm(B,X,W.t()); #print(Y)           # [ [3. 3. 3.] ]

# Слой как класс
import torch.nn as nn

X = torch.ones(N,  nX)  # (1,2)
# Создание слоя без весов и смещения, чисто из размерности
# Где nX - размер входного слоя; nY - размер выходного слоя
fc =  nn.Linear(nX, nY) # (2, 3) - класс слоя
# передача на вход матрицы X (2 размер по 1 оси и 2 размер у fc совпадают)
Y = fc(X)
# При формировании слоя не были указаны веса и смещение из-за чего они 
# определяются сами и в таком случае тензор будет иметь случайные значения
#print(Y)        # tensor([[s s s]],grad_fn=<Addmm>), где s - случайное значение
#print(fc.weight, fc.bias)   # весы и смещение

# Класс слоя объявляется свойство requires_grad=True всем своим параметрам
# Из-за чего строятся градиенты у этих параметров
#print(fc.bias)            # tensor([s s s], requires_grad=True) - смещение
#print(fc.bias.is_leaf)    # True - тензор графа
#print(fc.bias.grad)       # None - нет градиента

# В случае когда Y будет участвовать в какой-нибудь функции и после backward будут
# будут вычислены все графы Y, а в них и есть смещение
# Функция ошибки - определяет насколько сильно предсказание модели(Y) 
# отличается от истинного значения(T - которое вычисляется при backward)
# То есть torch.sum(Y*Y) вычисляет скаляр(одно число) из суммы квадратов Y
# После обратного прохода вычисляется T - истина, т.к. весы и смещения и находят
# значения правильные из входных(основываясь на весах и смещении прогнозируются ошибки)
L = torch.sum(Y*Y)
L.backward()                # вычисление градиентов для fc.bias и fc.weight
#print(fc.weight.grad)       # tensor([s s s]) - градиент весов
#print(fc.bias.grad)         # tensor([s s s]) - градиент смещения

#endregion

#region ПОЛНОСВЯЗНЫЙ СЛОЙ

N, nX, nY = 1, 2, 3  
W = torch.ones(nY, nX)  # весы
B = torch.ones(nY)      # смещение

# nn.Linear(in_features, out_features, bias=True) - преобразует y = x*W.t() + b, где
# W(матрица весов) храниться в транспонированном виде, так чтобы выполнялось x*W
# x - основная матрица, а b - смещение

# В данном случае не заданы весы и смещения, поэтому если это необходимо, то нужно
# присваивать в no_grad оболочке
fc = nn.Linear(nX, nY)      
with torch.no_grad():
    fc.weight.copy_(W)          # устанавливаем матрицу весов
    fc.bias.copy_(B)            # устанавливаем вектор смещений

# Если необходимо не менять весы или смещение(то есть не менять при обучении), то :
# fc.weight.requires_grad = False или fc.bias.requires_grad = False

# Вот как устроена эта же функция в torch.nn.functional:
# она требует тензор, весы и не обязательное смещение при этом весы должны выполнять
# условие матричного умножения
# def linear(X, weight, bias=None):                 # смещение не обязательно
#     if X.dim() == 2 and bias is not None:         # если 2 оси и задано смещение
#         ret = torch.addmm(bias, X, weight.t())    # X@(w.t()) + b
#     else:
#         output = X.matmul(weight.t())
#         if bias is not None: output += bias
#         ret = output
#     return ret

#endregion

#region СЛОЙ ЭМБЕДДИНГА

# nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None,
# norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)

# Обязательными являются num_embeddings(размер массива) и embedding_dim(размерность)
# Принимает тензор с Long переменными

# создаст весы размерность (5,2)
emb = nn.Embedding(5, 2, padding_idx=0)
# X определяет индексы которые нужно выбрать из тензора весов
X = torch.tensor([0,2,1])
 
#print(emb.weight)   # tensor([ [s0 s0] [s1  s1] [s2  s2] [s3 s3] [s4 s4] ],
                    # requires_grad=True)
#print(emb(X))       # tensor([ [s0 s0] [s2  s2] [s1  s1] ],  grad_fn=<EmbeddingBackward0>)

# Параметры:

# paddind_idx - задает индексу постоянное значение нуля
# padding_idx=1 - значит в x[0][2] = [0 0] (т.к. emb.weight[0][2] = [0 0])
emb = nn.Embedding(5, 2, padding_idx=1)
#print(emb(X))       # tensor([ [s0 s0] [s2  s2] [0  0] ],  grad_fn=<EmbeddingBackward0>)
# Где это используется ? :
# Например в разговорных нейросетях:
# "Привет" => токены [2, 3]
# "Как дела" => токены [4, 5, 6] , то x выглядел бы так:
# [ [2 3 0] [4 5 6] ] - то есть там где 0 не будет происходить 
# вычислений, что очень удобно

# max_norm - задает максимальную длину вектора и если превышается, 
# то вектор перенормируется(меняет max_norm на ту длину которая превысила)
# По умолчанию это значение равно 2
# Формула для нахождения: N = math.sqrt(sum), где sum - сумма значений тензора
# Пример:
# emb(X) = [[ 0.5561, -0.1329],[ 0.3771, -0.0645],[ 0.3371, -0.3638]], то
# N[1] = math.sqrt(0.5561-0.1329) = 0.5718 и т.д.
emb = nn.Embedding(5, 2, max_norm=3)
#print(emb(X))   # tensor([ [s0 s0] [s1  s1] [s2  s2] [s3 s3] [s4 s4] ],
                # requires_grad=True)
# norm - выводит тензор с нормализованными значениями от 0 до 3(max_norm=3) для 
# каждого элемента по 1 оси emb(X)   
#print(torch.norm(emb(X),dim=1)) # [s s s], где s - значения нормализации
# Зачем нужна нормировка? :
# В тензоре могут быть 1000 значений скажем от 0 до 1 или от 1 до 1000
# Чтобы было удобнее с ними работать их нормируют в более комфортные значения
# Нормализация помогает привести данные к одинаковому масштабу

# scale_grad_by_freq - регулирует значения для индексов, то есть
# если true: часто встречающиеся индексы будут уменьшаться, а те что реже увеличиваться
# По итогу будет создан баланс между частыми и редкими индексами
weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6], [7, 8.1, 9]])
# from_pretrained - предварительно созданые векторы для быстрого использования:
# from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None,
# norm_type=2.0, scale_grad_by_freq=False, sparse=False)
# freeze=False - для обновления весов и градиентов
emb = nn.Embedding.from_pretrained(weight, freeze=False, scale_grad_by_freq=False)
input = torch.LongTensor([0, 1, 1])   # Создать emb по первому индексу weight
output = emb(input)
loss = output.sum()
loss.backward()
# [1 1 1] - [0] индекс встречается 1 раз, [2 2 2] - [1] индекс встречается 2 раза,
# [0 0 0] - [2] индекс встречается 0 раз (тут стоят единицы, т.к. loss очень простая) 
#print(emb.weight.grad)          # [ [1 1 1] [2 2 2] [0 0 0] ]

#endregion

#region СЛОЙ КОНВОЛЮЦИИ

# nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, 
# dilation=1, groups=1, bias=True, padding_mode='zeros') - слой применяется 
# в основном применяется для картинок высотой rows, шириной cols и с in_channels 
# цветовых каналов(тем не менее может применяться не только для изображений)

# На вход входит тензор (N, in_channels, rows,cols)
# На выходе (N, out_channels, rows_out, cols_out)
# По выходному проход фильтры out_channels размерами 
# kernel_size(2,2 - размерность, если kernel_size=2)
# Глубина определяется in_channels

# Параметры:
# padding - задает то на сколько нужно расширить картинку перед прохождением фильтров
# padding_mode - задает то чем будет заполняться эта область
# (zeros, reflect, replicate,circular)
# duration - задает расстояние между пикселями попадающими в фильтр(шаг)

# Пример:
X = torch.zeros(1,1,3,4)    # [ [ [0 0 0 0] [0 0 0 0] [0 0 0 0] ] ]
X[0,0,:,:2] = 1             # [ [ [1 1 0 0] [1 1 0 0] [1 1 0 0] ] ] 
#print(X)   # (1,1,3,4) - одна выборка(1), один канал(1), размер изображения (3,4)

# т.к. kernel_size=2, то фильтры будут размерами (2,2)
# stride по умолчанию (1,1), то есть шаг равен 1 по ширине и высоте
# без смещения
conv = nn.Conv2d(1,1,kernel_size=2,bias=False)
#print(conv)     # Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)

# Присваивание веса слою вручную(с no_grad, чтобы не считалось сразу, правило такое)
with torch.no_grad():
    conv.weight.copy_(torch.tensor([[-1.,1.], [-1.,1.]]))

# requires_grad=True т.к. это весы у них так всегда по умолчанию(если false, 
# то весы не обновляются, а значит модель не учиться)
#print( conv.weight )    # tensor([[-1.,1.], [-1.,1.]], requires_grad=True)
#print( conv.bias )      # смещение не объявляли
# Расчет выходного тензора(OUTnum - выходной, Hin - входной, K - размер фильтра):
# OUT1 = 1 - поскольку X по первой оси имеет 1, то есть за один прогон одно изображение
# OUT2 = 1 - поскольку у conv значение out_channels равно 1 - количество 
# фильтров(каналов) в слое
# Остальные по формуле: Hnum = (Hin-K[ind])/stride[0])+1
# Hin = (1,1,3,4); K = (2,2); stride = (1,1), то:
# OUT3 = ((Hin[2] - K[0])/1) +1 = ((3-2)/1) +1 = 2
# OUT4 = ((Hin[3] - K[1])/1) +1 = ((4-2)/1) +1 = 3
# Исходный тензор будет иметь размерность (1,1,2,3)
# [ [ [1 1 0 0] [1 1 0 0] [1 1 0 0] ] ]:
# 1 1 0 0
# 1 1 0 0
# 1 1 0 0
# [ [-1 1] [-1 1] ] :
# -1 1
# -1 1
# В таком случае считаем первое скалярное произведение:
# левое верхнее окно
# [ [-1 1] [-1 1] ] @ [ [1 1] [1 1] ] = 1*-1 + 1*1 + 1*-1 + 1*1 = -2+2 = 0 
# правее на 1
# [ [-1 1] [-1 1] ] @ [ [1 0] [1 0] ] = -1*1 + 1*0 + -1*1 + 1*0 = -2
# правее на 1
# [ [-1 1] [-1 1] ] @ [ [0 0] [0 0] ] = -1*0 + 1*0 + -1*0 + 1*0 = 0
# вниз и влево к краю
# [ [-1 1] [-1 1] ] @ [ [1 1] [1 1] ] = 1*-1 + 1*1 + 1*-1 + 1*1 = -2+2 = 0
# правее на 1
# [ [-1 1] [-1 1] ] @ [ [1 0] [1 0] ] = -1*1 + 1*0 + -1*1 + 1*0 = -2
# правее на 1
# [ [-1 1] [-1 1] ] @ [ [0 0] [0 0] ] = -1*0 + 1*0 + -1*0 + 1*0 = 0
# Итого: [ [ [ [0 -2 0] [0 -2 0] ] ] ]
#print( conv(X) )    # tensor([[[[0 -2 0] [0 -2 0]]]], grad_fn=<ConvolutionBackward0>)

#endregion

#region СЛОЙ RNN

# nn.RNN(input_size, hidden_size, num_layers=1, nonlinearity='tanh', bias=True,
# batch_first=False, dropout=0, bidirectional=False)
# Предназначен для обработки последовательных данных(текст, временные ряды, 
# аудиофайлы и т.д.)
# Вычисляет новое скрытое состояние по старому и новому(входному)

# Параметры:
# input_size - размерность входа
# hidden_size - размерность выхода
# batch_first - задает индекс батча, что будет использоваться
# num_layers - задает размерность для первой оси скрытого слоя
# dropout - задает вероятность случайного забивания 0 в скрытых состояниях
# формы для того, чтобы избежать переобучения(при 1 вероятно не будеть учиться)
# biddirrectional - если true, то двунаправленная иначе однонаправленная:
# однонапрвленная - сеть генерирует данные по одному направлению к другому
# двунаправленна - сеть генерирует данные от одного к другому и к другому к одному
# (слева направо с права на лево). В таком случае скрытые состояния удваиваются, но
# и сеть становиться более мощной

# Размерность входа(seq_len, batch_size, input_size), 
# где seq_len - число RNN ячеек(входов)

# Слой возвращает все выходы формы(seq_len, batch_size, hidden_size)
# и последнее скрытое состояние формы(num_layers, batch_size, hidden_size)

X_dim  = 2           # размерность входов             = dim(x)
H_dim  = 4           # размерность скрытого состояния = dim(h)
L      = 3           # число входов (ячеек RNN слоя) - сколько векторов на вход
B      = 1           # число примеров (batch_size)

# Создание слоя 2 размерности входов и 4 размерностью скрытого состояния
# Если надо bidirectional=True, то применить к H0 на первой оси 2, а не 1 иначе ошибка
# поскольку при bidirectional=True существет уже два направления по скрытным слоям
rnn = nn.RNN(X_dim, H_dim)

# Входы для слоя:
X  = torch.zeros(L, B, X_dim)   # (3,1,2) [ [ [0 0] ] [ [0 0] ] [ [0 0] ] ]
H0 = torch.zeros(1, B, H_dim)   # (1,1,4) [ [ [0 0 0 0] ] ]

# На выходе: Y - выход; Hn - выход скрытого состояния
# X -> Y = (3,1,2) -> (3,1,4) (взяла 4 с HO, где (1,1,4)), т.к. на каждом 
# из 3 временных шагов(L) будет создано скрытое состояние размерности 4
# Hn = H0 = (1,1,4) = (1,1,4)
# Hn - просто запоминает последнее действие(размерность с которой работал)
Y, Hn = rnn(X, H0)                      # H0 не обязательно, по умолчанию и так 0

#print(tuple(Y.shape), tuple(Hn.shape))  #  (3, 1, 4) (1, 1, 4)
# Последнее значение Y и Hn совпадает, поскольку Hn = последнему присвоенному Y вектору
#print(Y)
#print(Hn)

#endregion

#region СЛОЙ LSTM

# torch.nn.LSTM(input_size, hidden_size, num_layers=1, nonlinearity='tanh', bias=True,
# batch_first=False, dropout=0, bidirectional=False)
# Абсолютный аналог RNN, который использует добавления памяти ячеек
# Ячейка памяти обновляется каждый раз при проходе с учетом текущего входа, 
# предыдущего скрытного состояния и состояния ячейки
# Хоть ячейка и обновляется на каждом временном шаге, тем не менее оно сохраняет 
# долгосрочную информацию для улушения обучения

X_dim  = 10          # размерность входов             = dim(x)
H_dim  = 20          # размерность скрытого состояния = dim(h)
L      = 3           # число входов (ячеек RNN слоя) - сколько векторов на вход
B      = 1           # число примеров (batch_size)

rnn = nn.LSTM(X_dim, H_dim)

X  = torch.zeros(L, B, X_dim)
h0 = torch.zeros(1, B, H_dim)
c0 = torch.zeros(1, B, H_dim)

output, (hn, cn) = rnn(X, (h0, c0))
#print(output)   # тензор с выходами для каждого шага
#print(hn)       # скрытое состояние(такое же как в RNN)
#print(cn)       # тензор состояния ячейки на последнем шаге

# Где используется ? 
# По сравнению с RNN имеет большее влияние по отношению к зависимостям 
# на долгих временных промежутках

#endregion

#region СЛОЙ MULTIHEADATTENTION

# nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, 
# add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)
# Основная задача определить какие части входных данных наиболее важны 
# для текущего элемента

# MultiheadAttention(Q,K,V), где: 
# Q - запрос для которого расчитывается внимание
# K - представление всех элементов последовательности
# V - информация извлекаемая из K
# На вход Q(L,N,E), K и V -> (S,N,E), где: 
# L - длина последовательности на которую обращено внимание(Q - target)
# N - размер батча(количество последовательностей обрабатываемых одновременно)
# E - размерность эмбединга(равная embed_dim) - определяет сколько информации будет 
# кодировано для каждого элемента (длина элементов-значений)
# S - длина последовательности K и V(к которым применино внимание)
# На выход (L,N,E) с весами (N,L,S)

# Весы Q(E,E), K(E,kdim), V(E,vdim), где: 
# kdim, vdim - параметры, что можно задать, но по умолчанию их нет
# (в таком случае равны E)

# Размерность каждой головы равна head_dim = E // num_heads
# На выходе получаются весы равные слою Linear(E,E,bias=bias), если есть bias, то 
# он добавляется для кадого из весов Q,K и V(у каждого свой)

# Если эмбединг входы(размерности тензоров одинаковы), то матрицы упакованы в 
# in_proj_weight формы (3*E, E) иначе это параметры: 
# q_proj_weight, k_proj_weight, v_proj_weight у самого MultiheadAttention
# Смещение(если есть) это in_proj_bias формы(3*E,), а на выход out_proj (там же)
# То есть: nn.MultiheadAttention(E,L).q_proj_weight и т.д.

L = 4  # Длина целевой последовательности(Q)
S = 5  # Длина входной последовательности(K и V)
N = 2  # Размер батча(количество примеров=batch_size)
E = 8  # Размер эмбеддинга(количество на выходе значений=embed_dim)

attention = nn.MultiheadAttention(E,L)

Q = torch.randn(L,N,E)
K = torch.randn(S,N,E)
V = torch.randn(S,N,E)

output, attn_weight = attention(Q,K,V)

# Output = (L,N,E) = (4,2,8)
#print(output)
# Weight = (N,L,S) = (2,4,5):
# N - было столько-то примеров
# L - из такой-то длины значений
# S - которых взяли отсюда
#print(attn_weight)

# Пример реализации многоголового внимания(ключевой компонент в трансформерах 
# типа BERT и GPT), работает только когда L = S -> 5 = 5:
E = 10 # E должна делится на L(то есть embed_dim/num_heads = True)
L = 5

attention = nn.MultiheadAttention(E,L)

Q = torch.randn(L,N,E)
K = torch.randn(S,N,E)
V = torch.randn(S,N,E)

output, attn_weight = attention(Q,K,V)

# Создаются веса для Q,K,V в переменной in_proj_weight:
#print(attention.in_proj_weight)
# Создаются если(bias=True) смещения для Q,K,V в переменной in_proj_bias:
#print(attention.in_proj_bias)

# достанем их для каждого из значений Q,K,V:
q_proj_weight = attention.in_proj_weight[:E,:]
k_proj_weight = attention.in_proj_weight[E:E*2,:]
v_proj_weight = attention.in_proj_weight[E*2:,:]

#endregion

#region СЛОЙ TRANSFORMERENCODERLAYER

# nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, 
# dropout=0.1, activation='relu')
# Это базовый слой трансформера, используемый в архитектурах трансформеров(это часть 
# более сложной структуры, например, BERT и GPT)

# Параметры:
# embed_dim(d_model) - размерность входных(выходных) эмбедингов(стандарт 512 или 1024)
# то есть на входе должен быть тензор размерность (...,...,512) и т.д.
# Почему такие большие ? 
# Другие люди пробовали разные размерности и выяснили что 512 и 1024 наиболее 
# стабильные и балансные
embed_dim = 512
# num_heads(nhead) - количество голов MULTIHEADATTENTION, то есть модель будет изучать
# сразу несколько взаимосвязей между токинами в последовательности
num_heads = 8
# feedforward_dim - размерность скрытого слоя в полносвязной сети, как работает:
# скажем на вход дали d_model = 512, то поступают векторы именно такой размерности,
# векторы проходят через головы MULTIHEADATTENTION(nhead=8) и результат передается 
# в полносвязную сеть, где размерность равна feedforward_dim = 2048 и уже после 
# активации(работы с этими значениями) она снова преобразуется в вектор размерности 
# d_model = 512, то есть feedforward_dim - это как-бы проходной слой
feedforward_dim = 2048 

encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,
                                           dim_feedforward=feedforward_dim)

# Пример входных данных: 10 временных шагов, 32 батча, размерность 512
x = torch.rand(1, 2, embed_dim)

output = encoder_layer(x)
# output = (1,2,embed_dim) = (1,2,512)
#print(output)       # два элемента по 512 значений
#print(output.shape)

#endregion

#region СЛОЙ TRANSFORMERENCODER

# nn.TransformerEncoder(encoder_layer, num_layers, norm=None)
# Данный слой состоит из нескольких слоев TRANSFORMERENCODERLAYER, которые состоят из
# слоев MULTIHEADATTENTION и полносвязной сети

E = 512             # размерность эмбединга
num_layers = 6      # число повторяющихся слоев TRANSFORMERENCODERLAYER(будет 
# создано 6 слоев энкодера трансформера, пройдет через них)

# Создание слоя TRANSFORMERENCODERLAYER без dim_feedforward(по умолчанию 2048)
encoder_layer       = nn.TransformerEncoderLayer(d_model=E, nhead=8)
# Создание слоя при помощи другого слоя(сложная сеть получается)
transformer_encoder = nn.TransformerEncoder     (encoder_layer, num_layers=6)

src = torch.rand(1, 2, E)
out = transformer_encoder(src)   
#print(src.shape, out.shape)     # out.shape == src.shape

#endregion

#region ВЫПРЕМЛЕНИЕ ТЕНЗОРА

# nn.Flatten(start_dim=1, end_dim=-1)
# По умолчанию сливает все индексы в один вектор, то есть преобразует 2D и более 
# массивы в одномерные вектора(1,1)

X = torch.ones(1,5,4,3)         
# (2,5,4,3) -> (2, 5*4*3) -> (2,60)
Y = torch.nn.Flatten()(X)       
#print(tuple(Y.shape))                   # (2, 60)

#endregion

#region КОСИНУС МЕЖДУ ВЕКТОРАМИ

# nn.CosineSimilarity(dim=1, eps=1e-08)
# Считает косинус между двумя векторами при этом соблюдая правила:
# Оба входа имеют размерность (*, D, *), где D - должны быть равны у обоих векторов
# Output будет иметь размерность (*,*)
# То есть если будет (*,D,*,*) -> (*,*,*) - D теряется, а D - всегда вторая ось

u, v = torch.randn(100, 3,4, 128), torch.randn(100, 3,4, 128)
output = nn.CosineSimilarity(dim=1, eps=1e-6) (u, v)
#print(output.shape)

#endregion

#region РАССТОЯНИЕ МЕЖДУ ВЕКТОРАМИ

# nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)
# Вычисляет расстояние между векторами, принимая на вход две матрицы (N,D), где 
# D - размерность вектора
# На выходе выдает (N) или (N,1) - если keepdim = True
# Применяется для сравнения пар данных

# Параметры:
# p=2.0 - евклидов расстояние или p=1.0 - манхетонское расстояние - способы 
# нахождения расстояния между точками в пространстве(они разные)
# eps=1e-06 - прибавляется к результату вычисления для того, чтобы улучшить 
# читабельность(чтобы не было цифр с e, как тут)
# keepdim - определяет сохранят ли размерности тензоров после вычисления расстояния
# true - будет иметь ту же размерность что и у входа 
# false - будет иметь уменьшенный результат(ненужное будет убрано)

u, v = torch.tensor([ [1,2,3]]), torch.tensor([ [4,5,6] ])
output = nn.PairwiseDistance(keepdim=False) (u,v)

# При keepdim=True равно (1,1)
# При keepdim=False равно (1,)
# output при u = [1 2 3] v = [ [4 5 6] ]:
# output[0] = sqrt((u[0]-v[0])^2 + (u[1]-v[1])^2 + (u[2]-v[2])^2) = 
# sqrt((1-4)^2 + (2-5)^2 + (3-6)^2) = sqr(9+9+9) = sqr(27) = 5,19
#print(output)       # 4,24
#print(output.shape) # (1,)

#endregion

#region ОТКЛЮЧЕНИЕ НЕЙРОНОВ

# nn.Dropout(p=0.5, inplace=False)
# Заменяет значения в тензоре с вероятностью p(1 - 100%, 0 - 0%) на 0 и 
# делит их на 1-p(это в режиме тренировки). Режимы:
# nn.Dropout.train() - режим тренировки
# nn.Dropout.eval() - режим оценивания(ничего не делает)
# Из-за деления 1-р среднее значение по модулю элементов в режиме тренировки 
# будет равно такому же как и в режиме оценивания

m = nn.Dropout(p = 0.2) #
o = torch.ones(10)      # [1 1 1 1 1 1 1 1 1]
# Каждая 1 = 1/1-p = 1/1-0.2 = 1/0.8 = 1,25
# При этом 20% значений будут равны 0
#print(m(o))    #  Пример: [1.25, 1.25, 1.25, 0.0, 1.25, 1.25, 1.25, 0.0, 1.25, 1.25]
 
x = torch.randn(100, 100)
 
m.eval()                        # меняем на режим оценивания
y = m(x)
# float() - преобразует булевый тензор(типа [True False ...]) в х[1 0 ...],
# то есть true=1, false=0
# mean - вычисляет среднее значение во всем элементам преобразованного тензора
# В режиме оценивания тензоры не меняются
#print((x==y).float().mean())    # 1 - не меняется, другое - меняется

# Зачем нужен этот режим ? 
# Скажем я хочу протестировать тензор по нейроной сети и чтобы значения были четкими
# я могу провести их через этот режим, чтобы они и в сети не становились нулями
# то есть это как бы отключает dropout у других слоев

m.train()
y=m(x)

# abs - перевод в модуль значения(убирает минус, результат всегда положительный)
# (x*x < 1e-10) и (y*y < 1e-10) вычисляют тензоры значений, которые близки к нулю
# квадрат элемента меньше одного(10^-10)
# Доказывает то, что изменяется только y, а не x
#print(x.abs().mean(),   (x*x < 1e-10).float().mean()  ) # 0.8025     0.    - 0% нулей
#print(y.abs().mean(),   (y*y < 1e-10).float().mean()  ) # 0.8001     0.206 - 20% нулей

#endregion

#region НОРМАЛИЗАЦИЯ ТЕНЗОРА

# nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, 
# track_running_stats=True)
# Вычисляет по первому индексу средние значения и дисперсии по каждому независимо

# На вход тензор формы(N,F) или (N,F,L), где:
# F = num_features
# Для тензора (N,F,L) усреднение происходит для осей 0 и 2, то есть для [0...L-1]
# Примеры: (1,1,2) -> (1,2-1); (4,4,5) -> (4,5-1); (7,5,4,2) -> (7,4-1,2-1)

# Параметры:
# momentum - предназначен для сглаживания средних значений

m =  nn.BatchNorm1d(4)
 
x = torch.tensor([[1.0, 2.0, 3.0, 4.0],[2.0, 3.0, 4.0, 5.0],[3.0, 4.0, 5.0, 6.0]])

# var - вычисляет дисперсию(чем больше тем значения более рассеяные относительно 
# среднего), считается по формуле: ((xi-ui)^2)/n-1, где:
# ui - среднии значения по тензору
# xi - сами значения столбцов
#print(x.mean(dim=0), x.var(dim=0))      # [2 3 4 5] и [1 1 1 1]

# Пример того как это работает:
# [ [1 2 3 4] [2 3 4 5] [3 4 5 6] ] = [ [5 7 9 11] [7 9 11 13] [9 11 13 15] ]
x = 2 * x + 3
#print(x)                                # [ [5 7 9 11] [7 9 11 13] [9 11 13 15] ]
# x.mean() = ui (по столбикам, не по строкам):
# u1 = 5+7+9/3 = 21/3 = 7
# u2 = 7+9+11/3 = 27/3 = 9
# u3 = 9+11+13/3 = 33/3 = 11
# u4 = 11+13+15/3 = 39/3 = 13 
# Итого средне равно [7 9 11 13]
# x.var() = ( (xi-ui) ^2) / (n-1), тогда (xi-ui) равны
# при этом тут считается неполная дисперсия (та у которой n-1, а не просто n)
# [0] = [5-7 7-7 9-7] = [-2 0 2]
# [1] = [7-9 9-9 11-9] = [-2 0 2]
# [2] = [9-11 11-11 13-11] = [-2 0 2]
# [3] = [11-13 13-13 15-13] = [-2 0 2]
# Итого (xi-ui) = [-2 0 2], тогда:
# ( (xi-ui) ^2) = [-2^2 0^2 2^2] = [4 0 4]
# ((xi-ui)^2) / (n-1) = [4 0 4] / (3-1) = 4+0+4/2 = 4 - для каждого, где 
# n - количество значений в тензоре
# x.var() = [4 4 4 4]
#print(x.mean(dim=0), x.var(dim=0))      # [7 9 11 13] и [4 4 4 4]
 
# Первое что делает нормализация это ищет u от x = [7 9 11 13] v = [4 4 4 4]
# Далее формула y = Y * (x-u)/(sqrt(v^2+e)) + B, где
# Y и B  - обучаемые параметры, которые по умолчанию равны 1 и 0
# e - добавка для численной стабильности
# (x-u) = [-2 0 -2] - посчитали ранее
# Считаем дисперсию v^2 = (( (xi-ui) ^2) / (n)):
# [4 0 4] / (3) = 4+0+4/3 = 8/3 = 2,66, то есть v^2 = [2,66 2,66 2,66 2,66]
# (sqrt(v^2+e)) = sqrt(2,66 + 1e-5) = sqrt(2,66) = 1,63
# (x-u)/(sqrt(v^2+e))= [-2 0 2]/1,63 = [-1,22 0 1,22]
# Y * (x-u)/(sqrt(v^2+e)) + B = 1*[-1,22 0 1,22] + 0 = [-1,22 0 1,22], то
# Y = [ [-1,22 -1,22 -1,22 -1,22] [0 0 0 0] [1,22 1,22 1,22 1,22] ]
y = m(x)
#print(y)

# После нормализации значения пришли к одним знаменателям, от этого и получается,
# что они уравнялись по среднему и дисперсии
# with torch.no_grad():
#     # [1.5895e-07  1.5895e-07 0.0000e+00 0.0000e+00] и [1.5 1.5 1.5 1.5]
#     print(y.mean(dim=0), y.var(dim=0))  

# state_dict - хранит словарь со всеми значениями и параметрами слоя
# for k, v in m.state_dict().items():     # weight             :(4,) - весы
#     print(f'{k:20s}: {tuple(v.shape)}') # bias               :(4,) - смещение
#                                         # running_mean       :(4,) - значение u
#                                         # running_var        :(4,) - значение v^2
#                     # num_batches_tracked:(  ) - число обработанных батчей((  ) = 1)

#endregion

#region MAXPOOL2D

# nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, 
# return_indices=False, ceil_mode=False)
# 

# [ [ [ [1 2 3 4] [5 6 7 8] [9 10 11 12] [13 14 15 16] ] ] ] (1,1,4,4)
x = torch.tensor([[[[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12],[13, 14, 15, 16]]]],
                 dtype=torch.float32)

# kernel_size = 2, значит окно будет формы (2,2)
# stride = 2, значит шаг по высоте и ширине равен 2
pool = nn.MaxPool2d(kernel_size=2, stride=2)

# Как найти y:
#  1  2  3  4
#  5  6  7  8
#  9 10 11 12
# 13 14 15 16
# Фильтр размерами (2,2), то 
# [0] берет [1 2 5 6] - левое верхнее окно
# Делает 2 шага вправо, то [1] берет [3 4 7 8] - правое верхнее окно
# Делает 2 шага влево и вниз, то [2] берет [9 10 13 14] - левое нижнее окно
# Делает 2 шага вправо, то [3] берет [11 12 15 16] - правое нижнее окно
# Из этих массивов выбираются наиболее большие и формируется тензор y:
# [1 2 5 6] = 6; [3 4 7 8] = 8; [9 10 13 14] = 14; [11 12 15 16] = 16
# y = [[[[6 8 14 16]]]] (1,1,1,4)
y = pool(x)
#print(y)    # [6 8 14 16]

#endregion

#region AVGPOOL2D

# nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, 
# count_include_pad=True, divisor_override=None)
# Работает схоже с MAXPOOL2D за исключением того, что считывает среднее значение

# [ [ [ [1 2 3 4] [5 6 7 8] [9 10 11 12] [13 14 15 16] ] ] ] (1,1,4,4)
x = torch.tensor([[[[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12],[13, 14, 15, 16]]]],
                 dtype=torch.float32)

# kernel_size = 2, значит окно будет формы (2,2)
# stride = 2, значит шаг по высоте и ширине равен 2
pool = nn.AvgPool2d(kernel_size=2, stride=2)

# y = [[[[3.5 5.5 11.5 13.5]]]]
y = pool(x)
#print(y)

#endregion

#region АКТИВАЦИОННЫЕ ФУНКЦИИ

# nn.Sigmoid()
# Сжимает значения в диапозон от 0 до 1(делает как бы вероятность) по формуле:
# S(x) = 1/ (1+e^-x), где e - основание натурального логорифма равного 2,71(константа)

x = torch.tensor([[1.0, -1.0, 0.5], [2.0, -3.0, 4.0]])
sigmoid = nn.Sigmoid()
# Посчитаем хотя бы одно:
# x[0] = 1/(1+e^-x[0]) = 1/(1+2,71^-1) = 1/(1+0,36) = 1/1,36 = 0,73
y = sigmoid(x)
#print(y)

# nn.Tanh()
# Сжимает значения в диапозоне от -1 до 1 по формуле:
# T(x) = (e^x - e^-x) / (e^x + e^-x)

x = torch.tensor([[1.0, -1.0, 0.5], [2.0, -3.0, 4.0]])
tanh = nn.Tanh()

# Расчитаем хотя бы одно:
# x[0] = (2,71^1 - 2,71^-1) / (2,71^1 + 2,71^-1) = 2,71 - 0,36 / 2,71 + 0,36 = 
# 2,35 / 3,07 = 0,76 
y = tanh(x)
#print(y)

# nn.ReLU(inplace=False)
# Сжимает значения по диапозону от 0 до +бесконечности по формуле
# R(x) = max(0,x), где 
# если x>0, то x = x
# если x<0, то x = 0
# То есть она просто преобразует все отрицательные значения в ноль, а 
# положительные не изменяет

x = torch.tensor([[1.0, -1.0, 0.5], [2.0, -3.0, 4.0]])
relu = nn.ReLU()

# [ [1 -1 0.5] [2 -3 4] ] -> [ [1 0 0.5] [2 0 4] ]
y = relu(x)
#print(y)

# nn.ELU(alpha=1.0, inplace=False)
# Действет так же как Relu за исключением того что когда x<0, то идет по немного 
# другой формуле, которая предотвращает проблему у Relu с мертвыми нейронами
# E(x) = x если x>=0
# E(x) = a(e^x - 1) если x<0, где a = alpha(1 по умолчанию)

x = torch.tensor([[1.0, -1.0, 0.5], [2.0, -3.0, 4.0]])
elu = nn.ELU(alpha=1.0)

# Расчитаем хотя бы одно отрицательное:
# x[1] = a(e^x-1) = 1(2,71^-1-1) = 0,36-1 = -0.63
y = elu(x)
#print(y)

# nn.Softmax(dim=None)
# Сжимает входной вектор в вероятности при этом сумма выходного вектора будет равна 1
# Формула: S(xi) = e^xi / (e^xj), где 
# (e^xj) - сумма экспонентов всех элементов тензора
# Каждый xi лежит в диапозоне от 0 до 1 и сумма всех элементов равна 1

x = torch.tensor([[1.0, 2.0, 3.0]])
softmax = nn.Softmax(dim=1)

# Для первой строки равной [1 2 3]:
# e^xj = [e^1 e^2 e^3] = [2,718 7,389 20,086]
# (e^xj).sum() = 2,718 + 7,389 + 20,086 = 30,193
# e^xi, то :
# [0] = e^1 / 30,193 = 2,71/30,193 = 0,09
# [1] = e^2 / 30,193 = 7,389/30,193 = 0,244
# [2] = e^3 / 30,193 = 20,086/30,193 = 0,665
# Их сумма : 0,09 + 0,244 + 0,665 = 1
y = softmax(x)
#print(y)
#print(y.sum(dim=1))  # Проверяем, что сумма по строкам равна 1

# nn.LogSoftmax(dim=None)
# В отличие от Softmax использует также логарифм при вычислении, что позволяет 
# избежать переполнений при вычислении экспонента
# Формула: LS(xi) = xi - log(e^xj)

x = torch.tensor([[1.0, 2.0, 3.0]])
log_softmax = nn.LogSoftmax(dim=1)

# Для первой строки [1 2 3] то:
# [2,718 7,389 20,086] и e^xj = 30,19 взял из прошлого
# Логарифм тут не натуральный а общий(ln), где не e а 10 
# [0] = xi - log(e^xj) = 1 - log(30,19) = 1 - 3,4 = -2,4
# [1] = 2 - log(30,19) = 2 - 3,4 = -1,4
# [2] = 3 - log(30,19) = 3 - 3,4 = -0,4
# Итого y = [-2,4 -1,4 -0,4]
# Сумма y = -2,4 -1,4 -0,4 = -4,2
y = log_softmax(x)
#print(y)
#print(y.sum(dim=1))

#endregion

#region ФУНКЦИЯ СРЕДНЕКВАДРАТИЧНОЙ ОШИБКИ b

# nn.MSELoss(size_average=None, reduce=None, reduction='mean')
# Использует для вычислений функцию потерь на основе средеквадратичной ошибки
# Часто применяется в задачах регресии

# Формула MSE = 1/N * ((yi - y`i)^2), где
# N - общее число элементов
# yi - истинное значение
# y`i - предсказанное значение

# Параметры:
# reduction - определяет способ агрегации(объединения всех в одну систему):
# 'mean' - возвращает среднее значение
# 'sum' - возвращает сумму квадратов ошибок
# 'none' - возвращает ошибки для каждого элемента без агрегации
 
y_true = torch.tensor([1.0, 2.0, 3.0])
y_pred = torch.tensor([1.5, 2.5, 3.5])
mse_loss = torch.nn.MSELoss()

# Расчитаем при N=3; yi = [1.0, 2.0, 3.0]; y`i = [1.5, 2.5, 3.5]
# MSE = 1/N * ((yi - y`i)^2)
# 1/N = 1/3 = 0,33
# (yi - y`i) = [1.5, 2.5, 3.5] - [1.0, 2.0, 3.0] = [0.5 0.5 0.5]
# ((yi - y`i)^2) = [-0.5 -0.5 -0.5]^2 = [0.25 0.25 0.25]
# 1/N * ((yi - y`i)^2) = 0,33 * [0.25 0.25 0.25] = 0,33 * 0,75 = 0,25
# Итого MSE = 0,25
loss = mse_loss(y_true, y_pred)

#print(loss)                                 # 0,25
# Если поменять параметр "reduction" вот, что будет
#print(loss == ((y_true-y_pred)**2).mean())  # reduction = 'mean' -> 0,25
#print(((y_true-y_pred)**2).sum())           # reduction = 'sum'  -> 0,75
#print((y_true-y_pred)**2)                   # reduction = 'none' -> [0.25 0.25 0.25]

#endregion

#region ФУНКЦИЯ БИНАРНОЙ КРОСС-ЭНТРОПИИ

# nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')
# Вычисляет функцию потерь для бинарных значений(для тензоров со значениями от 0 до 1)

# Формула: BCE(y,y`) = - 1/N * [yi * log(y`i) + (1-yi) * log(1-y`i)], где
# N - общее число элементов
# yi - истинное значение
# y`i - предсказанное значение

y_true = torch.tensor([1.0, 0.0, 1.0])
y_pred = torch.tensor([0.9, 0.2, 0.8])
bce_loss = torch.nn.BCELoss()

# Расчитаем если N = 3; yi = [1.0, 0.0, 1.0]; y`i = [0.9, 0.2, 0.8], то:
# - 1 * [yi * log(y`i) + (1-yi) * log(1-y`i)]/N
# yi * log(y`i) + (1-yi) * log(1-y`i):
# [0] = 1*log(0.9) + (1-1) * log(1-0,9) = 1*-0,1 + 0 = -0,1
# [1] = 0*log(0.2) + (1-0) * log(1-0,2) = 0 + 1*log(0,8) = 1*-0,22 = -0,22
# [2] = 1*log(0.8) + (1-1) * log(1-0.8) = -0,22 + 0 = -0,22
# yi * log(y`i) + (1-yi) * log(1-y`i) = [-0,1 -0,22 -0,22]
# - 1 * [yi * log(y`i) + (1-yi) * log(1-y`i)] = 
# -[-0,1 -0,22 -0,22] = [0,1 0,22 0,22]
# - 1 * [yi * log(y`i) + (1-yi) * log(1-y`i)]/ N = (0,1 + 0,22 + 0,22)/3 =
# 0,54/3 = 0,18
# ВНИМАНИЕ !!! : тут по другому стоят значения истины и предсказаний(обрати внимание)
loss = bce_loss(y_pred,y_true)
#print(loss)

#endregion

#region ФУНКЦИЯ КРОСС-ЭНТРОПИЯ

# nn.CrossEntropyLoss(weight=None,size_average=None,ignore_index=-100,reduce=None, 
# reduction='mean')
# Функция потерь для многоклассовой классификации, которая комбинирует бинарную 
# кросс-энтропию и softmax(активация)
# Формула: CEL= -log(e^xy/e^xi), где
# xy - предсказания для классов без softmax, то есть это не должны быть 
# значения от 0 до 1(функция сама это сделает)
# xi - индексы для всех классов(истинные), должны быть целыми числами, представляющими 
# индексы классов, а не значения векторов

# Входные данные могут иметь формы:
# xy = (B,C):float, а xi = (B,):long
# xy = (B,C,L1,...,Ln):float, xi = (B,L1,...,Ln):long 
logits = torch.tensor([[1.0, 2.0, 3.0], [1.5, 2.5, 3.5]], dtype=torch.float32) # (2,3)
labels = torch.tensor([2, 1], dtype=torch.long) # (2,)

w = torch.tensor([0.5, 1.0, 10.0])  # у меня ничего не изменялось из-за них 
# при смене значений
# ignore_index - будет игнорировать класс с индексом 1
criterion = nn.CrossEntropyLoss(weight=w, ignore_index=1)

# Посчитаем для первого:
# e^xi:
# [0] = 2,71^1 = 2,71; [1] = 2,71^2 = 7,34; [2] = 2,71^3 = 19,9
# e^xi = [2,71 7,34 19,9] = 2,71 + 7,34 + 19,9 = 29,95
# e^xy/e^xi:
# [0] = e^1/e^xi = 2,71/29,95 = 0,09
# [1] = 7,34/29,95 = 0,24
# [2] = 19,9/29,95 = 0,66 
# Итого e^xy/e^xi = [0,09 0,24 0,66]
# -log(e^xy/e^xi):
# [0], где 2, то для первого [0,09 0,24 0,66] выбираем третий индекс:
# CEL1 = -log(e^xy/e^xi[2]) = -log(0,66) = 0,41
# Итого loss[0] = 0,41
loss = criterion(logits, labels)
#print(loss)            
#print(criterion.weight)

#endregion

#region ФУНКЦИЯ ЛОГАРИФМИЧЕСКИЙ МАКСИМУМ ПРАВДОПОДОБНОСТИ

# nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, 
# reduction='mean')
# Используется после КРОСС-ЭНТРОПИИ для минимизации логарифма вероятности правильного
# класса(максимизирует правдоподобие)
# Формула: NLLLOSS(y,y`) = (-yi * log(yi`)) /N, где
# yi - истинная метка класса
# yi` - предсказанная вероятность(от LogSoftMax)
# N - число примеров

# На вход принимает обработанные через LogSoftMax логиты
# 

logits = torch.tensor([[1.0, 2.0, 3.0], [1.5, 2.5, 3.5]], dtype=torch.float32)
labels = torch.tensor([2, 1], dtype=torch.long)

log_softmax = nn.LogSoftmax(dim=1) 
# Для первой оси это [-2,4 -1,4 -0,4]
log_probs = log_softmax(logits) 

criterion = nn.NLLLoss(ignore_index=1)
# Расчитаем для первого, если yi = [2 1] yi`= [-2,4 -1,4 -0,4]:
# для первого логита [-2,4 -1,4 -0,4] мы берем 2 индекс([2 1]), то:
# [0] = -(-0,4)/1 = 0,4
loss = criterion(log_probs, labels)
print(loss)

#endregion

#region



#endregion

#region



#endregion