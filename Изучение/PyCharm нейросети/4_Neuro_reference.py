# ССЫЛКА НА ИСТОЧНИК: https://qudata.com/ml/ru/NN_Base_Torch_NN_Ref.html

#region ИНТЕРФЕЙСЫ ДЛЯ СЛОЕВ(ГОТОВЫЕ)

import torch
# Слои могут быть функциями или классами

# Модуль который содержит функции для слоев(много)
import  torch.nn.functional as F 
# Слой как функция:
N, nX, nY = 1, 2, 3                               # число примеров, входов, выходов
X = torch.ones(N,  nX)                            # [ [1 1] ] - примеры
# Пояснение для весов:
# nY - размерность выходного тензора
# nX - размерность входного тензора
W = torch.ones(nY, nX)                            # [ [1 1] [1 1] [1 1] ] - весы
B = torch.ones(nY)                                # [ 1 1 1 ] - смещение

# linear - преобразует тензор X с весами W и смещением B по формуле:
# Y = X*W.t() + B
# W.t() = [ [1 1 1] [1 1 1] ] - транспонированная версия W(нужна для умножения матриц)
# X*W.t() = (1,2)*(2,3) = (1,3) = [ [1 1] ] * [ [1 1 1] [1 1 1] ] = 
# [ 1*1+1*1 1*1+1*1 1*1+1*1] = [2 2 2]
# X*W.t() + B = [ [2 2 2] ] + [ 1 1 1 ] = [ [3 3 3] ]
Y = F.linear(X, W, B); #print(Y)                 # [ [3. 3. 3.] ]
# Аналоги этой функции:
Y = X.mm(W.t())+B; #print(Y)                    # [ [3. 3. 3.] ]
Y = torch.addmm(B,X,W.t()); #print(Y)           # [ [3. 3. 3.] ]

# Слой как класс
import torch.nn as nn

X = torch.ones(N,  nX)  # (1,2)
# Создание слоя без весов и смещения, чисто из размерности
# Где nX - размер входного слоя; nY - размер выходного слоя
fc =  nn.Linear(nX, nY) # (2, 3) - класс слоя
# передача на вход матрицы X (2 размер по 1 оси и 2 размер у fc совпадают)
Y = fc(X)
# При формировании слоя не были указаны веса и смещение из-за чего они 
# определяются сами и в таком случае тензор будет иметь случайные значения
#print(Y)        # tensor([[s s s]],grad_fn=<Addmm>), где s - случайное значение
#print(fc.weight, fc.bias)   # весы и смещение

# Класс слоя объявляется свойство requires_grad=True всем своим параметрам
# Из-за чего строятся градиенты у этих параметров
#print(fc.bias)            # tensor([s s s], requires_grad=True) - смещение
#print(fc.bias.is_leaf)    # True - тензор графа
#print(fc.bias.grad)       # None - нет градиента

# В случае когда Y будет участвовать в какой-нибудь функции и после backward будут
# будут вычислены все графы Y, а в них и есть смещение
# Функция ошибки - определяет насколько сильно предсказание модели(Y) 
# отличается от истинного значения(T - которое вычисляется при backward)
# То есть torch.sum(Y*Y) вычисляет скаляр(одно число) из суммы квадратов Y
# После обратного прохода вычисляется T - истина, т.к. весы и смещения и находят
# значения правильные из входных(основываясь на весах и смещении прогнозируются ошибки)
L = torch.sum(Y*Y)
L.backward()                # вычисление градиентов для fc.bias и fc.weight
#print(fc.weight.grad)       # tensor([s s s]) - градиент весов
#print(fc.bias.grad)         # tensor([s s s]) - градиент смещения

#endregion

#region ПОЛНОСВЯЗНЫЙ СЛОЙ

N, nX, nY = 1, 2, 3  
W = torch.ones(nY, nX)  # весы
B = torch.ones(nY)      # смещение

# nn.Linear(in_features, out_features, bias=True) - преобразует y = x*W.t() + b, где
# W(матрица весов) храниться в транспонированном виде, так чтобы выполнялось x*W
# x - основная матрица, а b - смещение

# В данном случае не заданы весы и смещения, поэтому если это необходимо, то нужно
# присваивать в no_grad оболочке
fc = nn.Linear(nX, nY)      
with torch.no_grad():
    fc.weight.copy_(W)          # устанавливаем матрицу весов
    fc.bias.copy_(B)            # устанавливаем вектор смещений

# Если необходимо не менять весы или смещение(то есть не менять при обучении), то :
# fc.weight.requires_grad = False или fc.bias.requires_grad = False

# Вот как устроена эта же функция в torch.nn.functional:
# она требует тензор, весы и не обязательное смещение при этом весы должны выполнять
# условие матричного умножения
# def linear(X, weight, bias=None):                 # смещение не обязательно
#     if X.dim() == 2 and bias is not None:         # если 2 оси и задано смещение
#         ret = torch.addmm(bias, X, weight.t())    # X@(w.t()) + b
#     else:
#         output = X.matmul(weight.t())
#         if bias is not None: output += bias
#         ret = output
#     return ret

#endregion

#region СЛОЙ ЭМБЕДДИНГА

# nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None,
# norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)

# Обязательными являются num_embeddings(размер массива) и embedding_dim(размерность)
# Принимает тензор с Long переменными

# создаст весы размерность (5,2)
emb = nn.Embedding(5, 2, padding_idx=0)
# X определяет индексы которые нужно выбрать из тензора весов
X = torch.tensor([0,2,1])
 
#print(emb.weight)   # tensor([ [s0 s0] [s1  s1] [s2  s2] [s3 s3] [s4 s4] ],
                    # requires_grad=True)
#print(emb(X))       # tensor([ [s0 s0] [s2  s2] [s1  s1] ],  grad_fn=<EmbeddingBackward0>)

# Параметры:

# paddind_idx - задает индексу постоянное значение нуля
# padding_idx=1 - значит в x[0][2] = [0 0] (т.к. emb.weight[0][2] = [0 0])
emb = nn.Embedding(5, 2, padding_idx=1)
#print(emb(X))       # tensor([ [s0 s0] [s2  s2] [0  0] ],  grad_fn=<EmbeddingBackward0>)
# Где это используется ? :
# Например в разговорных нейросетях:
# "Привет" => токены [2, 3]
# "Как дела" => токены [4, 5, 6] , то x выглядел бы так:
# [ [2 3 0] [4 5 6] ] - то есть там где 0 не будет происходить 
# вычислений, что очень удобно

# max_norm - задает максимальную длину вектора и если превышается, 
# то вектор перенормируется(меняет max_norm на ту длину которая превысила)
# По умолчанию это значение равно 2
# Формула для нахождения: N = math.sqrt(sum), где sum - сумма значений тензора
# Пример:
# emb(X) = [[ 0.5561, -0.1329],[ 0.3771, -0.0645],[ 0.3371, -0.3638]], то
# N[1] = math.sqrt(0.5561-0.1329) = 0.5718 и т.д.
emb = nn.Embedding(5, 2, max_norm=3)
#print(emb(X))   # tensor([ [s0 s0] [s1  s1] [s2  s2] [s3 s3] [s4 s4] ],
                # requires_grad=True)
# norm - выводит тензор с нормализованными значениями от 0 до 3(max_norm=3) для 
# каждого элемента по 1 оси emb(X)   
#print(torch.norm(emb(X),dim=1)) # [s s s], где s - значения нормализации
# Зачем нужна нормировка? :
# В тензоре могут быть 1000 значений скажем от 0 до 1 или от 1 до 1000
# Чтобы было удобнее с ними работать их нормируют в более комфортные значения
# Нормализация помогает привести данные к одинаковому масштабу

# scale_grad_by_freq - регулирует значения для индексов, то есть
# если true: часто встречающиеся индексы будут уменьшаться, а те что реже увеличиваться
# По итогу будет создан баланс между частыми и редкими индексами
weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6]])
# from_pretrained - предварительно обученная модель с заданными параметрами:
# from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None,
# norm_type=2.0, scale_grad_by_freq=False, sparse=False)
emb = nn.Embedding.from_pretrained(weight)    # Создание слоя с готовыми векторами
input = torch.LongTensor([1])                 # Создать emb по первому индексу
#print(emb(input))                             # tensor([[ 4.0,  5.1,  6.0]])


#endregion

#region



#endregion

#region



#endregion