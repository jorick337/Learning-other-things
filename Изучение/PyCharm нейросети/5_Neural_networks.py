# ССЫЛКА НА ИСТОЧНИК: https://qudata.com/ml/ru/NN_Base_Torch_NN.html

#region ПОСЛЕДОВАТЕЛЬНОСТЬ СЛОЕВ

import torch
import torch.nn as nn
 
nX = 2  # вход
nH = 5  # промежуток(скрытый слой)
nY = 1  # выход

# Squential - контейнер для последовательного комбинирования слоев модели, где 
# каждый слой будет применяться по порядку к данным. Это удобный способ создания модели
model = nn.Sequential(
          nn.Linear(nX, nH),    # входной слой
          nn.Sigmoid(),         # активация скрытого слоя
          nn.Linear(nH, nY),    # выходной слой
          nn.Sigmoid() )        # активация выходного слоя
# То есть 2 -> 5 -> 1

#endregion

#region ФУНКЦИОНАЛЬНАЯ АРХИТЕКТУРА

# Более удобным считается создание нейроной сети в функциональном виде, наследуя
# класс nn.Module, где:
# __init__ - определяются переменные сети которые будут использоваться
# forward - тут строиться архитектура сети которая будет использоваться при этом слои
# складываются в вычислительный граф

nX = 2  # вход
nH = 5  # промежуток(скрытый слой)
nY = 1  # выход

class TwoLayersNet(nn.Module):
    def __init__(self, nX, nH, nY):        
        super(TwoLayersNet, self).__init__()     # конструктор предка с этим именем
         
        self.fc1 = nn.Linear(nX, nH)             # создаём параметры модели
        self.fc2 = nn.Linear(nH, nY)             # в полносвязных слоях

    def forward(self, x):                        # задаётся прямой проход
        x = self.fc1(x)                          # выход первого слоя
        x = nn.Sigmoid()(x)                      # активация
        x = self.fc2(x)                          # выход второго слоя
        x = nn.Sigmoid()(x)                      # активация
        return x

model = TwoLayersNet(nX, nH, nY)                    # экземпляр сети

#endregion

#region МОДЕЛЬНЫЕ ДАННЫЕ

# Создадим два класса на 2D плоскости со значениями от 0 до 1:
# X - все случайные значения
# Y - те что находяться в центре
X = torch.tensor([[0.4214, 0.2862],[0.8847, 0.3323],[0.4405, 0.6170],
                  [0.2672, 0.2194],[0.4839, 0.0361],[0.6251, 0.9900]])
# Посчитаем для одного:
# [0.4214, 0.2862] = [(0.4214 -0,5)^2 (0.2862-0,5)^2] = [0,08^2 0,22^2] = 
# [0,0064 0,0484] = [0,0548]
# [0,0548] < 0,1 == true == 1
# [0.8847, 0.3323] = [(0,8847-0,5)^2 (0,33-0,5)^2] = [0,38^2 0,17^2] = 
# [0,1444] [0,0289] = [0,1733]
# [0,1733] < 0,1 == false == 0
# Где 1 то в центре, 0 наоборот
Y = (torch.sum((X - 0.5)**2, axis=1) < 0.1).float().view(-1,1)

# matplotlib.pyplot - библиотека которая поможет нарисовать, чтобы было понятнее
import matplotlib.pyplot as plt                      

plt.figure (figsize=(5, 5))                                  # размеры (квадрат)
plt.scatter(X.numpy()[:,0], X.numpy()[:,1], c=Y.numpy()[:,0], s=30, 
            cmap=plt.cm.Paired, edgecolors='k')        
# plt.show()                                                   # выводим рисунок    

#endregion

#region ОБУЧЕННЫЕ СЕТИ

# Для обучения необходимы: функция ошибки(BSELoss) и оптимизатор(SGD)
model = TwoLayersNet(2, 5, 1)       
 
loss      = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.5, momentum=0.8)        

# fit - функция обучающая модель(эпоха обучения), где
# X - тензор с входными значениями
# Y - тензор с истиными значениями
# batch_size - количество примеров
# train - включать ли режим тренировки или просто посмотреть как работает 
# обученная модель
def fit(model, X,Y, batch_size=100, train=True):    
    model.train(train)              # важно для Dropout, BatchNorm
    sumL = 0                        # ошибка
    sumA = 0                        # точность
    # примеры(батчи) - длина X, деленная на количество примеров(получается 
    # целочисленный тип Long)
    numB = int( len(X)/batch_size ) 
    
    for i in range(0, numB*batch_size, batch_size):          
        # Простой выбор нужного примера
        # xb = X[i: i+batch_size]                     # текущий пример
        # yb = Y[i: i+batch_size]                     # и его истина
           
        # Выборка происходит случайно, при этом выбираются только нужные для данного
        # примера значения, НО в обучение могут попасть не все примеры, а иногда даже 
        # одинаковые. Этот метод используется в классе DataLoader(параметр shuffle)
        idx = torch.randint(high = len(X), size = (batch_size,) )
        xb = X[idx]
        yb = Y[idx]
                      
        y = model(xb)                               # прямое распространение
        L = loss(y, yb)                             # вычисляем ошибку

        # Пример вставки своего оптимизатора
        # if train:                                   # в режиме обучения
        #     L.backward()                            # вычисляем градиенты            
        #     with torch.no_grad():                   
        #         for p in model.parameters():        # для каждого параметра модели
        #             p.add_(p.grad, alpha=-0.7)      # p += -0.7*grad
        #             p.grad.zero_()                  # обнуляем градиенты
  
        if train:                                   # в режиме обучения
            optimizer.zero_grad()                   # обнуляем градиенты        
            L.backward()                            # вычисляем градиенты            
            optimizer.step()                        # подправляем параметры
                                     
        sumL += L.item()                            # суммарная ошибка (item из графа)
        # sumA - берут и округляют y, где предсказания и сравнивают с истиной, находя
        # среднее значение точности предсказания значения
        sumA += (y.round() == yb).float().mean()    
         
    return sumL/numB,  sumA/numB                    # средняя ошибка и точность


X = torch.rand (1200,2)                                         # вход
Y = (torch.sum((X - 0.5)**2, axis=1) < 0.1).float().view(-1,1)  # истина

# режим оценки модели(проверяем до цикла что там):
# print("before:      loss: %.4f accuracy: %.4f" %  fit(model, X,Y, train=False))

# Что за модель ? 
# Мы даем ей на вход 1200 парных значений которые пройдя через нее должны выдать нам
# только те значения которые находяться в центре
epochs = 1000
# for epoch in range(epochs):     # каждая эпоха проходит по всем параметрам
#     # чтобы улучшить обучение можно перемешать данные(в данном случае создается новая
#     # память, что не всегда хорошо, поэтому смотри что в самой функции fit есть 
#     # перемешка)
#     # idx = torch.randperm( len(X) )
#     # X = X[idx]
#     # Y = Y[idx]
    
#     L,A = fit(model, X, Y)      # эпоха выводит среднюю ошибку и точность
    
#     # вывод для удобства через каждые 100 эпох
#     if epoch % 100 == 0 or epoch == epochs-1:                 
#         print(f'epoch: {epoch:5d} loss: {L:.4f} accuracy: {A:.4f}' )   

#endregion

#region



#endregion

#region



#endregion

#region



#endregion